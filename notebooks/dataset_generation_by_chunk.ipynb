{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset generation by chunk\n",
    "\n",
    "1. Imports\n",
    "2. Load linear regression parameters from json file\n",
    "3. Generate dataset with choosen chunks (number of iterations)\n",
    "\n",
    "\n",
    "## --- Notes ---\n",
    "+ You may not assign a variable to the generate_dataset() method, because it doesn't mantain anything in memory\n",
    "+ You must specify also a path and a saver (possibly an implementation of Result_Saver) that will save generated files in disk\n",
    "+ Choose a right amount of chunks to balance memory and cpu usage \n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Chunks = 50\n",
    "+ json params has n_points = 50, so 50 iterations will generate 1 tuple at a time\n",
    "+ The process will take low memory, but high cpu usage and generation time (see generation verbose output for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Adding dgpy package to python path, otherwise it will fail. You don't have to do that if it's installed as a package\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../\"))\n",
    "\n",
    "import dgpy\n",
    "from dgpy.model.lr_params import LR_Params\n",
    "from dgpy.persistence.json_loader import Json_Params_Loader\n",
    "from dgpy.persistence.csv_saver import Csv_Results_Saver \n",
    "from dgpy.generator.lr_generator import LR_Generator, LR_Chunk_Generator\n",
    "\n",
    "filename = os.path.join(os.getcwd(), \"data_tests/params.json\")\n",
    "dirpath = os.path.join(os.getcwd(), \"data_tests_chunk\")\n",
    "\n",
    "j_loader = Json_Params_Loader(filename)\n",
    "params = j_loader.load('lr') # if you don't specify param_type will load with param_type='lr' default option  \n",
    "\n",
    "# print\n",
    "print(\"filename: {}\".format(filename))\n",
    "attrs = vars(params)\n",
    "print(\"\".join(\"%s: %s\\n\" % item for item in attrs.items()))\n",
    "\n",
    "generator = LR_Chunk_Generator()\n",
    "saver = Csv_Results_Saver(dirpath)\n",
    "chunks = 50\n",
    "generator.generate_dataset(params, saver, chunks=chunks, verbose=True)\n",
    "# you can specify other generation parameters, see \"dataset_generation.ipynb\" for optimization levels and \"saving_results.ipynb\" for compression types\n",
    "\n",
    "# go to dir path to see results"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Chunks = 1\n",
    "+ json params has n_points = 50, so 1 iterations will generate 50 tuple at a time\n",
    "+ The process will take high memory, but low cpu usage and generation time (see generation verbose output for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Adding dgpy package to python path, otherwise it will fail. You don't have to do that if it's installed as a package\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../\"))\n",
    "\n",
    "import dgpy\n",
    "from dgpy.model.lr_params import LR_Params\n",
    "from dgpy.persistence.json_loader import Json_Params_Loader\n",
    "from dgpy.persistence.csv_saver import Csv_Results_Saver \n",
    "from dgpy.generator.lr_generator import LR_Generator, LR_Chunk_Generator\n",
    "\n",
    "filename = os.path.join(os.getcwd(), \"data_tests/params.json\")\n",
    "dirpath = os.path.join(os.getcwd(), \"data_tests_chunk\")\n",
    "\n",
    "j_loader = Json_Params_Loader(filename)\n",
    "params = j_loader.load('lr') # if you don't specify param_type will load with param_type='lr' default option  \n",
    "\n",
    "# print\n",
    "print(\"filename: {}\".format(filename))\n",
    "attrs = vars(params)\n",
    "print(\"\".join(\"%s: %s\\n\" % item for item in attrs.items()))\n",
    "\n",
    "generator = LR_Chunk_Generator()\n",
    "saver = Csv_Results_Saver(dirpath)\n",
    "chunks = 1\n",
    "generator.generate_dataset(params, saver, chunks=chunks, verbose=True)\n",
    "# you can specify other generation parameters, see \"dataset_generation.ipynb\" for optimization levels and \"saving_results.ipynb\" for compression types\n",
    "\n",
    "# go to dir path to see results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}